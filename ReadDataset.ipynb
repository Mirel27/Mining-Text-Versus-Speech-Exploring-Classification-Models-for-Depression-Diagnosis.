{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               unique_id  \\\n",
      "0      000884bdc57c9b1474b85832d52d7e0cf4ea0e02a58793...   \n",
      "1      00095ed1a1f295a3caf4358c8c5559ffa52cff72e62d81...   \n",
      "2      0009b77b45ae261711ce288c5bb5c8575c667cdfc400b2...   \n",
      "3      000dba296259c778164201b5fcac764aa2b65f4628179c...   \n",
      "4      00115b3389cd8a3d16102527f23e9ec819afdef3d16666...   \n",
      "...                                                  ...   \n",
      "18673  ffdb574be94c53845b980a55cbb2d9ce0880c3246bdf24...   \n",
      "18674  ffe21929c28b432efaa4bffc2785e42354b08aa2971d46...   \n",
      "18675  ffe2acf9e7d9422eaeec6bc4e6a592283421a11f8761ff...   \n",
      "18676  fff7d16c33a2dfd37eeea5dbe7a5b003b8b43d4b0b417e...   \n",
      "18677  fffde19ceafc7b4eb1ee93a9907fd8b9ee9053293d6e5b...   \n",
      "\n",
      "                                       concatenated_body  \n",
      "0      I like how the Red Tie Legion chose a picture ...  \n",
      "1      I used to love to mix addys and Valium 10s tho...  \n",
      "2      Thanks for the advice man ! Have a great New Y...  \n",
      "3      The simple ones feel elegant. The simple ones ...  \n",
      "4      Hopefully \"new games\" doesn't mean \"new ways t...  \n",
      "...                                                  ...  \n",
      "18673  i follow her owners on ig and she actually get...  \n",
      "18674  Congrats on becoming the owner of this cat! Wh...  \n",
      "18675  I was impressed by your company in the initial...  \n",
      "18676  thanks for the warning :) thanks for the warni...  \n",
      "18677  That’s good to know- thank you! That’s good to...  \n",
      "\n",
      "[18678 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "# Read the Parquet file and create a Pandas DataFrame\n",
    "file_path = 'non_depressed.parquet'\n",
    "data_frame = pd.read_parquet(file_path)\n",
    "\n",
    "# Specify the columns to check for the value 1\n",
    "columns_to_check = ['excitement', 'gratitude', 'joy', 'love', 'optimism', 'pride']\n",
    "\n",
    "# Create a mask to filter rows where any of the specified columns have a value of 1\n",
    "mask = data_frame[columns_to_check].apply(lambda row: any(row == 1), axis=1)\n",
    "\n",
    "# Apply the mask and select rows that match the conditions\n",
    "selected_rows = data_frame[mask]\n",
    "\n",
    "# Drop rows with missing author\n",
    "selected_rows = selected_rows.dropna(subset=[\"author\"])\n",
    "\n",
    "# Drop the rows that have author = [deleted] or text = [removed]\n",
    "selected_rows = selected_rows[(selected_rows[\"author\"] != \"[deleted]\") & (selected_rows[\"text\"] != \"[removed]\")]\n",
    "\n",
    "# Define a function to generate a unique ID using SHA-256 hash\n",
    "def generate_unique_id(data):\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    sha256_hash.update(data.encode('utf-8'))\n",
    "    hex_digest = sha256_hash.hexdigest()\n",
    "    return hex_digest\n",
    "\n",
    "# Apply the unique ID generation function to 'author' column\n",
    "selected_rows[\"unique_id\"] = selected_rows[\"author\"].apply(generate_unique_id)\n",
    "\n",
    "# Drop the 'author' column\n",
    "selected_rows = selected_rows.drop('author', axis=1)\n",
    "\n",
    "selected_rows.count()\n",
    "# Group by 'unique_id' and concatenate 'text' column values\n",
    "concatenated_df = selected_rows.groupby('unique_id')['text'].apply(' '.join).reset_index(name='concatenated_body')\n",
    "\n",
    "# Show the resulting DataFrame with concatenated 'text' values\n",
    "print(concatenated_df)\n",
    "\n",
    "# Print the count of unique IDs\n",
    "#print(\"Number of unique IDs:\", concatenated_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and rename the 'concatenated_body' column\n",
    "concatenated_df = concatenated_df[['concatenated_body']].rename(columns={'concatenated_body': 'value'})\n",
    "\n",
    "# Create a new column 'label' with value 0\n",
    "concatenated_df['label'] = 0\n",
    "\n",
    "# Select only the 'label' and 'value' columns\n",
    "final_df = concatenated_df[['label', 'value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write the final DataFrame to a Parquet file\n",
    "output_parquet_file = 'non_depressed.parquet'\n",
    "final_df.to_parquet(output_parquet_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "# Read the Parquet file and create a Pandas DataFrame\n",
    "file_path = 'depression.parquet'\n",
    "data_frame = pd.read_parquet(file_path)\n",
    "\n",
    "# Drop rows with missing author\n",
    "DepressionDataset = DepressionDataset.dropna(subset=[\"author\"])\n",
    "\n",
    "#Drop the rows that have author = [deleted]\n",
    "DepressionDataset = DepressionDataset.where(col(\"author\") != \"[deleted]\")\n",
    "DepressionDataset = DepressionDataset.where(col(\"body\") != \"[removed]\")\n",
    "\n",
    "\n",
    "def generate_unique_id(data):\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    sha256_hash.update(data.encode('utf-8'))\n",
    "    hex_digest = sha256_hash.hexdigest()\n",
    "    return hex_digest\n",
    "\n",
    "# Register the UDF to use it in Spark DataFrame\n",
    "generate_unique_id_udf = udf(generate_unique_id, StringType())\n",
    "\n",
    "# Use withColumn to create a new column 'unique_id' based on 'author' column\n",
    "DepressionDataset = DepressionDataset.withColumn(\"unique_id\", generate_unique_id_udf(\"author\"))\n",
    "\n",
    "#Delete unused columns\n",
    "DepressionDataset = DepressionDataset.drop('author')\n",
    " #Concatenate the \"body\" column for rows with the same ID\n",
    "\n",
    "\n",
    "# Group by 'uniqueid' and concatenate 'body' column values\n",
    "concatenated_df = DepressionDataset.groupBy('unique_id').agg(concat_ws(' ', collect_list('body')).alias('concatenated_body'))\n",
    "\n",
    "# Show the resulting DataFrame with concatenated 'body' values\n",
    "concatenated_df.show()\n",
    "\n",
    "\n",
    "\n",
    "# Print the DataFrame with the unique IDs\n",
    "concatenated_df.count()\n",
    "\n",
    "# Select and rename the 'concatenated_body' column\n",
    "concatenated_df = concatenated_df[['concatenated_body']].rename(columns={'concatenated_body': 'value'})\n",
    "\n",
    "# Create a new column 'label' with value 0\n",
    "concatenated_df['label'] = 1\n",
    "\n",
    "# Select only the 'label' and 'value' columns\n",
    "final_df = concatenated_df[['label', 'value']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write the final DataFrame to a Parquet file\n",
    "output_parquet_file = 'depressed.parquet'\n",
    "final_df.to_parquet(output_parquet_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
