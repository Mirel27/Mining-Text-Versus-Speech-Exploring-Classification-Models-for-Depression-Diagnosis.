{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (2.14.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (1.24.3)\n",
      "Requirement already satisfied: dill in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (0.16.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets>=2.0.0->evaluate) (12.0.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->evaluate) (2023.5.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mirel agy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance evaluation\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee170ca8cef4c6f87ea565cfe1ae488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "#import tensorflow_text as text\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "# Hugging Face Dataset\n",
    "from datasets import Dataset\n",
    "# Modeling\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback, TextClassificationPipeline\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "#model_path = \"mental/mental-bert-base-uncased\"\n",
    "\n",
    "df1 = pd.read_parquet(\"non_depressed_clean.parquet\")\n",
    "df2 = pd.read_parquet(\"depression_clean.parquet\")\n",
    "\n",
    "# Append df2 to df1\n",
    "df_subreddits = pd.concat([df1,df2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label     int32\n",
       "value    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['label'] = df1['label'].astype(int)\n",
    "\n",
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I like how the Red Tie Legion chose a picture ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I used to love to mix addys and Valium 10s tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Thanks for the advice man ! Have a great New Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>The simple ones feel elegant. The simple ones ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Hopefully \"new games\" doesn't mean \"new ways t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              value\n",
       "0      0  I like how the Red Tie Legion chose a picture ...\n",
       "1      0  I used to love to mix addys and Valium 10s tho...\n",
       "2      0  Thanks for the advice man ! Have a great New Y...\n",
       "3      0  The simple ones feel elegant. The simple ones ...\n",
       "4      0  Hopefully \"new games\" doesn't mean \"new ways t..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subreddits.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def preprocessing_df(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters and non-alphanumeric characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "# Clean the 'value' column using the clean_text function\n",
    "df_subreddits['value'] = df_subreddits['value'].apply(preprocessing_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3605 entries, 0 to 1705\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   3605 non-null   int32 \n",
      " 1   value   3605 non-null   object\n",
      "dtypes: int32(1), object(1)\n",
      "memory usage: 70.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    1899\n",
       "1    1706\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subreddits.info()\n",
    "df_subreddits['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "#amz_review = pd.read_csv( 'amazon_cells_labelled.txt', sep='\\t', names=['review', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset has 2884 records.\n",
      "The testing dataset has 721 records.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'appended_df' contains your dataset and 'target_column' is the column containing class labels\n",
    "X = df_subreddits.drop(columns=['label'])\n",
    "y = df_subreddits['label']\n",
    "\n",
    "# Stratified splitting with an 80-20 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create training and testing DataFrames\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Check the number of records in training and testing datasets.\n",
    "print(f'The training dataset has {len(train_data)} records.')\n",
    "print(f'The testing dataset has {len(test_data)} records.')\n",
    "\n",
    "hg_train_data = Dataset.from_pandas(train_data)\n",
    "hg_test_data = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at mental/mental-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "model_class, tokenizer_class, config_class = AutoModelForSequenceClassification,AutoTokenizer,AutoConfig\n",
    "\n",
    "path= \"mental/mental-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(path, num_labels = 2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='mental/mental-roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View tokenizer\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unknown token is <unk> and the ID for the unkown token is 3.\n",
      "The seperator token is </s> and the ID for the seperator token is 2.\n",
      "The pad token is <pad> and the ID for the pad token is 1.\n",
      "The sentence level classification token is <s> and the ID for the classification token is 0.\n",
      "The mask token is <mask> and the ID for the mask token is 50264.\n"
     ]
    }
   ],
   "source": [
    "# Mapping between special tokens and their IDs.\n",
    "print(f'The unknown token is {tokenizer.unk_token} and the ID for the unkown token is {tokenizer.unk_token_id}.')\n",
    "print(f'The seperator token is {tokenizer.sep_token} and the ID for the seperator token is {tokenizer.sep_token_id}.')\n",
    "print(f'The pad token is {tokenizer.pad_token} and the ID for the pad token is {tokenizer.pad_token_id}.')\n",
    "print(f'The sentence level classification token is {tokenizer.cls_token} and the ID for the classification token is {tokenizer.cls_token_id}.')\n",
    "print(f'The mask token is {tokenizer.mask_token} and the ID for the mask token is {tokenizer.mask_token_id}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af766668c1743f7b535232cd51fb3e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8cdce29c504b87b90944360cbe7235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/721 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Funtion to tokenize data\n",
    "def tokenize_dataset(data):\n",
    "    return tokenizer(data[\"value\"], \n",
    "                     max_length=32, \n",
    "                     truncation=True, \n",
    "                     padding=\"max_length\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "dataset_train = hg_train_data.map(tokenize_dataset)\n",
    "dataset_test = hg_test_data.map(tokenize_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['value', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2884\n",
      "})\n",
      "Dataset({\n",
      "    features: ['value', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 721\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the data\n",
    "print(dataset_train)\n",
    "print(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2884"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 'hello this year i went deep into depression i think my routine was a lot like yours waking up feeling hopeless thinking about the reasons not to kill myself that very day browsing this subreddit to feel myself validated and to know that im not the only one thinking like this but this subreddit is a downward spiral do yourself a favor if youre depressed and dont come to this subreddit seeing the phrase im gonna kill myself or its done for me doesnt make you feel validated it perpetuates the down spiral\\n\\nin june my mother asked me youre beyond your control arent you and i answered yes while crying she booked therapy for me because i didnt had the courage then from there life slowly started to be brighter \\ni found out that i have a depressive nature and that its ok its how i am and with therapy you can develop strategies to fight it or a better term accept it as a part of you\\ni hope that i never will come back to that state of mind but if i return i know ways to fight it\\n\\nif youre like me at the time just lurking seeing people saying that want to kill themselves and thinking yeah me too do yourself a favor close this subreddit and try to book therapy or talk to someone dont get lost in this rabbit hole\\n\\nthere is hope friends i love you all and i hope that you can find your happiness in the future\\n\\n\\nedit formatting',\n",
       " 'label': 1,\n",
       " '__index_level_0__': 123,\n",
       " 'input_ids': [0,\n",
       "  42891,\n",
       "  42,\n",
       "  76,\n",
       "  939,\n",
       "  439,\n",
       "  1844,\n",
       "  88,\n",
       "  6943,\n",
       "  939,\n",
       "  206,\n",
       "  127,\n",
       "  6108,\n",
       "  21,\n",
       "  10,\n",
       "  319,\n",
       "  101,\n",
       "  14314,\n",
       "  19841,\n",
       "  62,\n",
       "  2157,\n",
       "  24418,\n",
       "  2053,\n",
       "  59,\n",
       "  5,\n",
       "  2188,\n",
       "  45,\n",
       "  7,\n",
       "  3549,\n",
       "  2185,\n",
       "  14,\n",
       "  2],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./classification_transfer_learning_transformer/\",          \n",
    "    logging_dir='./classification_transfer_learning_transformer/logs',            \n",
    "    logging_strategy='epoch',\n",
    "    logging_steps=100,    \n",
    "    num_train_epochs=5,              \n",
    "    per_device_train_batch_size=32,  \n",
    "    per_device_eval_batch_size=32,  \n",
    "    learning_rate=2e-5,\n",
    "    seed=42,\n",
    "    save_strategy='epoch',\n",
    "    save_steps=100,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 138 evaluation models in Hugging Face.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['precision',\n",
       " 'code_eval',\n",
       " 'roc_auc',\n",
       " 'cuad',\n",
       " 'xnli',\n",
       " 'rouge',\n",
       " 'pearsonr',\n",
       " 'mse',\n",
       " 'super_glue',\n",
       " 'comet',\n",
       " 'cer',\n",
       " 'sacrebleu',\n",
       " 'mahalanobis',\n",
       " 'wer',\n",
       " 'competition_math',\n",
       " 'f1',\n",
       " 'recall',\n",
       " 'coval',\n",
       " 'mauve',\n",
       " 'xtreme_s',\n",
       " 'bleurt',\n",
       " 'ter',\n",
       " 'accuracy',\n",
       " 'exact_match',\n",
       " 'indic_glue',\n",
       " 'spearmanr',\n",
       " 'mae',\n",
       " 'squad',\n",
       " 'chrf',\n",
       " 'glue',\n",
       " 'perplexity',\n",
       " 'mean_iou',\n",
       " 'squad_v2',\n",
       " 'meteor',\n",
       " 'bleu',\n",
       " 'wiki_split',\n",
       " 'sari',\n",
       " 'frugalscore',\n",
       " 'google_bleu',\n",
       " 'bertscore',\n",
       " 'matthews_correlation',\n",
       " 'seqeval',\n",
       " 'trec_eval',\n",
       " 'rl_reliability',\n",
       " 'angelina-wang/directional_bias_amplification',\n",
       " 'cpllab/syntaxgym',\n",
       " 'kaggle/ai4code',\n",
       " 'codeparrot/apps_metric',\n",
       " 'mfumanelli/geometric_mean',\n",
       " 'poseval',\n",
       " 'brier_score',\n",
       " 'abidlabs/mean_iou',\n",
       " 'abidlabs/mean_iou2',\n",
       " 'giulio98/codebleu',\n",
       " 'mase',\n",
       " 'mape',\n",
       " 'smape',\n",
       " 'dvitel/codebleu',\n",
       " 'NCSOFT/harim_plus',\n",
       " 'JP-SystemsX/nDCG',\n",
       " 'Drunper/metrica_tesi',\n",
       " 'jpxkqx/peak_signal_to_noise_ratio',\n",
       " 'jpxkqx/signal_to_reconstruction_error',\n",
       " 'hpi-dhc/FairEval',\n",
       " 'nist_mt',\n",
       " 'lvwerra/accuracy_score',\n",
       " 'character',\n",
       " 'charcut_mt',\n",
       " 'ybelkada/cocoevaluate',\n",
       " 'harshhpareek/bertscore',\n",
       " 'posicube/mean_reciprocal_rank',\n",
       " 'bstrai/classification_report',\n",
       " 'omidf/squad_precision_recall',\n",
       " 'Josh98/nl2bash_m',\n",
       " 'BucketHeadP65/confusion_matrix',\n",
       " 'BucketHeadP65/roc_curve',\n",
       " 'yonting/average_precision_score',\n",
       " 'transZ/test_parascore',\n",
       " 'transZ/sbert_cosine',\n",
       " 'hynky/sklearn_proxy',\n",
       " 'unnati/kendall_tau_distance',\n",
       " 'r_squared',\n",
       " 'Viona/fuzzy_reordering',\n",
       " 'Viona/kendall_tau',\n",
       " 'lhy/hamming_loss',\n",
       " 'lhy/ranking_loss',\n",
       " 'Muennighoff/code_eval_octopack',\n",
       " 'yuyijiong/quad_match_score',\n",
       " 'Splend1dchan/cosine_similarity',\n",
       " 'AlhitawiMohammed22/CER_Hu-Evaluation-Metrics',\n",
       " 'Yeshwant123/mcc',\n",
       " 'transformersegmentation/segmentation_scores',\n",
       " 'sma2023/wil',\n",
       " 'chanelcolgate/average_precision',\n",
       " 'ckb/unigram',\n",
       " 'Felipehonorato/eer',\n",
       " 'manueldeprada/beer',\n",
       " 'tialaeMceryu/unigram',\n",
       " 'He-Xingwei/sari_metric',\n",
       " 'langdonholmes/cohen_weighted_kappa',\n",
       " 'fschlatt/ner_eval',\n",
       " 'hyperml/balanced_accuracy',\n",
       " 'brian920128/doc_retrieve_metrics',\n",
       " 'guydav/restrictedpython_code_eval',\n",
       " 'k4black/codebleu',\n",
       " 'Natooz/ece',\n",
       " 'ingyu/klue_mrc',\n",
       " 'Vipitis/shadermatch',\n",
       " 'unitxt/metric',\n",
       " 'gabeorlanski/bc_eval',\n",
       " 'jjkim0807/code_eval',\n",
       " 'vichyt/metric-codebleu',\n",
       " 'fastrepl/mean_reciprocal_rank',\n",
       " 'fastrepl/mean_average_precision',\n",
       " 'mtc/fragments',\n",
       " 'DarrenChensformer/eval_keyphrase',\n",
       " 'kedudzic/charmatch',\n",
       " 'agkhalil/ragene_metrics',\n",
       " 'eperezfmit/my_metric',\n",
       " 'eperezfmit/my_metric_3',\n",
       " 'mcnemar',\n",
       " 'exact_match',\n",
       " 'wilcoxon',\n",
       " 'kaleidophon/almost_stochastic_order',\n",
       " 'word_length',\n",
       " 'lvwerra/element_count',\n",
       " 'word_count',\n",
       " 'text_duplicates',\n",
       " 'perplexity',\n",
       " 'label_distribution',\n",
       " 'toxicity',\n",
       " 'regard',\n",
       " 'honest',\n",
       " 'ybelkada/toxicity',\n",
       " 'ronaldahmed/ccl_win',\n",
       " 'meg/perplexity',\n",
       " 'cakiki/tokens_per_byte',\n",
       " 'lsy641/distinct']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of evaluation modules\n",
    "print(f'There are {len(evaluate.list_evaluation_modules())} evaluation models in Hugging Face.\\n')\n",
    "\n",
    "# List all evaluation metrics\n",
    "evaluate.list_evaluation_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the metric\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    logits, labels = eval_pred\n",
    "    # probabilities = tf.nn.softmax(logits)\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mirel Agy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037e5f6947a443d9800b729a94cff13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.247, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c76012e14e4a70bd6c6dd85061c436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08266227692365646, 'eval_accuracy': 0.9764216366158114, 'eval_runtime': 69.0911, 'eval_samples_per_second': 10.436, 'eval_steps_per_second': 0.333, 'epoch': 1.0}\n",
      "{'loss': 0.0592, 'learning_rate': 1.2e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a77f03f9944af9bec584713263da60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10323011130094528, 'eval_accuracy': 0.9736477115117892, 'eval_runtime': 75.4581, 'eval_samples_per_second': 9.555, 'eval_steps_per_second': 0.305, 'epoch': 2.0}\n",
      "{'train_runtime': 1945.924, 'train_samples_per_second': 7.41, 'train_steps_per_second': 0.234, 'train_loss': 0.15308823952308068, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=182, training_loss=0.15308823952308068, metrics={'train_runtime': 1945.924, 'train_samples_per_second': 7.41, 'train_steps_per_second': 0.234, 'train_loss': 0.15308823952308068, 'epoch': 2.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    compute_metrics=compute_metrics,\n",
    "   callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f6d21718294719b2df7a08f5f9ea8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-2.9904017,  2.8293424],\n",
       "       [ 3.270552 , -2.8226159],\n",
       "       [ 3.2402349, -2.8625734],\n",
       "       ...,\n",
       "       [-2.9630318,  2.8111506],\n",
       "       [ 3.2539642, -2.7969892],\n",
       "       [ 3.266674 , -2.8195107]], dtype=float32), label_ids=array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0], dtype=int64), metrics={'test_loss': 0.08266227692365646, 'test_accuracy': 0.9764216366158114, 'test_runtime': 67.2558, 'test_samples_per_second': 10.72, 'test_steps_per_second': 0.342})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions\n",
    "y_test_predict = trainer.predict(dataset_test)\n",
    "\n",
    "# Take a look at the predictions\n",
    "y_test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.9904017,  2.8293424],\n",
       "       [ 3.270552 , -2.8226159],\n",
       "       [ 3.2402349, -2.8625734],\n",
       "       [ 3.2840462, -2.8510957],\n",
       "       [-2.9717913,  2.815731 ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted logits\n",
    "y_test_logits = y_test_predict.predictions\n",
    "\n",
    "# First 5 predicted probabilities\n",
    "y_test_logits[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
       "array([[0.00295958, 0.99704045],\n",
       "       [0.99774677, 0.00225316],\n",
       "       [0.9977684 , 0.00223159],\n",
       "       [0.9978393 , 0.00216074],\n",
       "       [0.0030562 , 0.9969438 ]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted probabilities\n",
    "y_test_probabilities = tf.nn.softmax(y_test_logits)\n",
    "\n",
    "# First 5 predicted logits\n",
    "y_test_probabilities[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted labels\n",
    "y_test_pred_labels = np.argmax(y_test_probabilities, axis=1)\n",
    "\n",
    "# First 5 predicted probabilities\n",
    "y_test_pred_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actual labels\n",
    "y_test_actual_labels = y_test_predict.label_ids\n",
    "\n",
    "# First 5 predicted probabilities\n",
    "y_test_actual_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e2250bd38d41228923032bbe7703c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.05030614137649536,\n",
       " 'eval_accuracy': 0.9840499306518724,\n",
       " 'eval_runtime': 284.8165,\n",
       " 'eval_samples_per_second': 10.126,\n",
       " 'eval_steps_per_second': 0.32,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer evaluate\n",
    "trainer.evaluate(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610d59b72feb4c08abd395753d3ff805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.08266227692365646,\n",
       " 'eval_accuracy': 0.9764216366158114,\n",
       " 'eval_runtime': 74.136,\n",
       " 'eval_samples_per_second': 9.725,\n",
       " 'eval_steps_per_second': 0.31,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer evaluate\n",
    "trainer.evaluate(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.9747399702823181}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load f1 metric\n",
    "metric_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Compute f1 metric\n",
    "metric_f1.compute(predictions=y_test_pred_labels, references=y_test_actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.9618768328445748}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load recall metric\n",
    "metric_recall = evaluate.load(\"recall\")\n",
    "\n",
    "# Compute recall metric\n",
    "metric_recall.compute(predictions=y_test_pred_labels, references=y_test_actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "tokenizer.save_pretrained('./sentiment_transfer_learning_transformer/mental_roberta/')\n",
    "\n",
    "# Save model\n",
    "trainer.save_model('./sentiment_transfer_learning_transformer/mental_roberta/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./sentiment_transfer_learning_transformer/mental_roberta/\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained('./sentiment_transfer_learning_transformer/mental_roberta/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import shap\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_path = \"./sentiment_transfer_learning_transformer/mental_roberta/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "\n",
    "# define a prediction function\n",
    "def f(x):\n",
    "    tv = torch.tensor(\n",
    "        [\n",
    "            tokenizer.encode(v, padding=\"max_length\", max_length= 512, truncation=True)\n",
    "            for v in x\n",
    "        ]\n",
    "    )\n",
    "    outputs = model(tv)[0].detach().cpu().numpy()\n",
    "    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
    "    val = sp.special.logit(scores[:, 1])  # use one vs rest logit units\n",
    "    return val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# build an explainer using a token masker\n",
    "explainer = shap.Explainer(f, tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "# explain the model's predictions on IMDB reviews\n",
    "#imdb_train = nlp.load_dataset(\"imdb\")[\"train\"]\n",
    "shap_values = explainer(hg_test_data['value'][1:10], fixed_context=1)\n",
    "\n",
    "shap_values\n",
    "\n",
    "shap.plots.bar(shap_values.abs.max(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
